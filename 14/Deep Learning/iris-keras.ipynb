{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow / Keras Iris Data set problem solution\n",
    "\n",
    "In the following Jupyter notebook file we will create a visual representation of the Iris data set. I will walk you through this problem set as if you were a novice programmer.\n",
    "\n",
    "##### What is the Iris data set you ask?\n",
    "\n",
    "If you didn't read the readMe let me summarise it:\n",
    "\n",
    "The Iris data set contains 50 samples each of 3 different species of flowers:\n",
    "\n",
    "1. Iris Setosa\n",
    "2. Iris Virginica\n",
    "3. Iris Versicolor\n",
    "\n",
    "The data has 4 measurements from each sample:\n",
    "\n",
    "1. Sepal length\n",
    "2. Sepal Width\n",
    "3. Petal length\n",
    "4. Petal width\n",
    "\n",
    "If you'd like to know more go [Here](https://archive.ics.uci.edu/ml/datasets/iris)\n",
    "\n",
    "Lets get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Reference: https://github.com/emerging-technologies/keras-iris\n",
    "\n",
    "import csv\n",
    "import numpy as np\n",
    "import keras as kr "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Wait whats a TensorFlow?\n",
    "\n",
    "Well in mathematics, tensors are geometric objects that describe linear relations between geometric vectors, scalars, and other tensors. TensorFlow is an open source software library for numerical computation using data flow graphs. Nodes in the graph represent mathematical operations, while the graph edges represent the multidimensional data arrays (tensors) communicated between them. If you would like to know more about TensorFlow, I'd start [here](https://www.tensorflow.org/)\n",
    "\n",
    "###### What does that all mean?\n",
    "\n",
    "We're building a neural network, a neural network is a system of hardware and/or software patterned after the operations of neurons in the human brain. Neural networks also called artificial neural networks (ANN) are a variety of deep learning technologies. You can read up on neural networks [here](https://en.wikipedia.org/wiki/Artificial_neural_network) if you so wish!\n",
    "\n",
    "Ok firstly lets load up our data set!\n",
    "\n",
    "### Load the iris dataset into an list and then split the data into arrays\n",
    "\n",
    "To do this we're going to create a list that will contain all of our data. Then populate arrays using sub-elements of that list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 5.1  3.5  1.4  0.2]\n",
      " [ 4.9  3.   1.4  0.2]\n",
      " [ 4.7  3.2  1.3  0.2]\n",
      " [ 4.6  3.1  1.5  0.2]\n",
      " [ 5.   3.6  1.4  0.2]\n",
      " [ 5.4  3.9  1.7  0.4]\n",
      " [ 4.6  3.4  1.4  0.3]\n",
      " [ 5.   3.4  1.5  0.2]\n",
      " [ 4.4  2.9  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.1]\n",
      " [ 5.4  3.7  1.5  0.2]\n",
      " [ 4.8  3.4  1.6  0.2]\n",
      " [ 4.8  3.   1.4  0.1]\n",
      " [ 4.3  3.   1.1  0.1]\n",
      " [ 5.8  4.   1.2  0.2]\n",
      " [ 5.7  4.4  1.5  0.4]\n",
      " [ 5.4  3.9  1.3  0.4]\n",
      " [ 5.1  3.5  1.4  0.3]\n",
      " [ 5.7  3.8  1.7  0.3]\n",
      " [ 5.1  3.8  1.5  0.3]\n",
      " [ 5.4  3.4  1.7  0.2]\n",
      " [ 5.1  3.7  1.5  0.4]\n",
      " [ 4.6  3.6  1.   0.2]\n",
      " [ 5.1  3.3  1.7  0.5]\n",
      " [ 4.8  3.4  1.9  0.2]\n",
      " [ 5.   3.   1.6  0.2]\n",
      " [ 5.   3.4  1.6  0.4]\n",
      " [ 5.2  3.5  1.5  0.2]\n",
      " [ 5.2  3.4  1.4  0.2]\n",
      " [ 4.7  3.2  1.6  0.2]\n",
      " [ 4.8  3.1  1.6  0.2]\n",
      " [ 5.4  3.4  1.5  0.4]\n",
      " [ 5.2  4.1  1.5  0.1]\n",
      " [ 5.5  4.2  1.4  0.2]\n",
      " [ 4.9  3.1  1.5  0.2]\n",
      " [ 5.   3.2  1.2  0.2]\n",
      " [ 5.5  3.5  1.3  0.2]\n",
      " [ 4.9  3.6  1.4  0.1]\n",
      " [ 4.4  3.   1.3  0.2]\n",
      " [ 5.1  3.4  1.5  0.2]\n",
      " [ 5.   3.5  1.3  0.3]\n",
      " [ 4.5  2.3  1.3  0.3]\n",
      " [ 4.4  3.2  1.3  0.2]\n",
      " [ 5.   3.5  1.6  0.6]\n",
      " [ 5.1  3.8  1.9  0.4]\n",
      " [ 4.8  3.   1.4  0.3]\n",
      " [ 5.1  3.8  1.6  0.2]\n",
      " [ 4.6  3.2  1.4  0.2]\n",
      " [ 5.3  3.7  1.5  0.2]\n",
      " [ 5.   3.3  1.4  0.2]\n",
      " [ 7.   3.2  4.7  1.4]\n",
      " [ 6.4  3.2  4.5  1.5]\n",
      " [ 6.9  3.1  4.9  1.5]\n",
      " [ 5.5  2.3  4.   1.3]\n",
      " [ 6.5  2.8  4.6  1.5]\n",
      " [ 5.7  2.8  4.5  1.3]\n",
      " [ 6.3  3.3  4.7  1.6]\n",
      " [ 4.9  2.4  3.3  1. ]\n",
      " [ 6.6  2.9  4.6  1.3]\n",
      " [ 5.2  2.7  3.9  1.4]\n",
      " [ 5.   2.   3.5  1. ]\n",
      " [ 5.9  3.   4.2  1.5]\n",
      " [ 6.   2.2  4.   1. ]\n",
      " [ 6.1  2.9  4.7  1.4]\n",
      " [ 5.6  2.9  3.6  1.3]\n",
      " [ 6.7  3.1  4.4  1.4]\n",
      " [ 5.6  3.   4.5  1.5]\n",
      " [ 5.8  2.7  4.1  1. ]\n",
      " [ 6.2  2.2  4.5  1.5]\n",
      " [ 5.6  2.5  3.9  1.1]\n",
      " [ 5.9  3.2  4.8  1.8]\n",
      " [ 6.1  2.8  4.   1.3]\n",
      " [ 6.3  2.5  4.9  1.5]\n",
      " [ 6.1  2.8  4.7  1.2]\n",
      " [ 6.4  2.9  4.3  1.3]\n",
      " [ 6.6  3.   4.4  1.4]\n",
      " [ 6.8  2.8  4.8  1.4]\n",
      " [ 6.7  3.   5.   1.7]\n",
      " [ 6.   2.9  4.5  1.5]\n",
      " [ 5.7  2.6  3.5  1. ]\n",
      " [ 5.5  2.4  3.8  1.1]\n",
      " [ 5.5  2.4  3.7  1. ]\n",
      " [ 5.8  2.7  3.9  1.2]\n",
      " [ 6.   2.7  5.1  1.6]\n",
      " [ 5.4  3.   4.5  1.5]\n",
      " [ 6.   3.4  4.5  1.6]\n",
      " [ 6.7  3.1  4.7  1.5]\n",
      " [ 6.3  2.3  4.4  1.3]\n",
      " [ 5.6  3.   4.1  1.3]\n",
      " [ 5.5  2.5  4.   1.3]\n",
      " [ 5.5  2.6  4.4  1.2]\n",
      " [ 6.1  3.   4.6  1.4]\n",
      " [ 5.8  2.6  4.   1.2]\n",
      " [ 5.   2.3  3.3  1. ]\n",
      " [ 5.6  2.7  4.2  1.3]\n",
      " [ 5.7  3.   4.2  1.2]\n",
      " [ 5.7  2.9  4.2  1.3]\n",
      " [ 6.2  2.9  4.3  1.3]\n",
      " [ 5.1  2.5  3.   1.1]\n",
      " [ 5.7  2.8  4.1  1.3]\n",
      " [ 6.3  3.3  6.   2.5]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 7.1  3.   5.9  2.1]\n",
      " [ 6.3  2.9  5.6  1.8]\n",
      " [ 6.5  3.   5.8  2.2]\n",
      " [ 7.6  3.   6.6  2.1]\n",
      " [ 4.9  2.5  4.5  1.7]\n",
      " [ 7.3  2.9  6.3  1.8]\n",
      " [ 6.7  2.5  5.8  1.8]\n",
      " [ 7.2  3.6  6.1  2.5]\n",
      " [ 6.5  3.2  5.1  2. ]\n",
      " [ 6.4  2.7  5.3  1.9]\n",
      " [ 6.8  3.   5.5  2.1]\n",
      " [ 5.7  2.5  5.   2. ]\n",
      " [ 5.8  2.8  5.1  2.4]\n",
      " [ 6.4  3.2  5.3  2.3]\n",
      " [ 6.5  3.   5.5  1.8]\n",
      " [ 7.7  3.8  6.7  2.2]\n",
      " [ 7.7  2.6  6.9  2.3]\n",
      " [ 6.   2.2  5.   1.5]\n",
      " [ 6.9  3.2  5.7  2.3]\n",
      " [ 5.6  2.8  4.9  2. ]\n",
      " [ 7.7  2.8  6.7  2. ]\n",
      " [ 6.3  2.7  4.9  1.8]\n",
      " [ 6.7  3.3  5.7  2.1]\n",
      " [ 7.2  3.2  6.   1.8]\n",
      " [ 6.2  2.8  4.8  1.8]\n",
      " [ 6.1  3.   4.9  1.8]\n",
      " [ 6.4  2.8  5.6  2.1]\n",
      " [ 7.2  3.   5.8  1.6]\n",
      " [ 7.4  2.8  6.1  1.9]\n",
      " [ 7.9  3.8  6.4  2. ]\n",
      " [ 6.4  2.8  5.6  2.2]\n",
      " [ 6.3  2.8  5.1  1.5]\n",
      " [ 6.1  2.6  5.6  1.4]\n",
      " [ 7.7  3.   6.1  2.3]\n",
      " [ 6.3  3.4  5.6  2.4]\n",
      " [ 6.4  3.1  5.5  1.8]\n",
      " [ 6.   3.   4.8  1.8]\n",
      " [ 6.9  3.1  5.4  2.1]\n",
      " [ 6.7  3.1  5.6  2.4]\n",
      " [ 6.9  3.1  5.1  2.3]\n",
      " [ 5.8  2.7  5.1  1.9]\n",
      " [ 6.8  3.2  5.9  2.3]\n",
      " [ 6.7  3.3  5.7  2.5]\n",
      " [ 6.7  3.   5.2  2.3]\n",
      " [ 6.3  2.5  5.   1.9]\n",
      " [ 6.5  3.   5.2  2. ]\n",
      " [ 6.2  3.4  5.4  2.3]\n",
      " [ 5.9  3.   5.1  1.8]]\n"
     ]
    }
   ],
   "source": [
    "#load the Iris dataset\n",
    "iris = list(csv.reader(open(\"./data/IRIS.csv\")))[1:]\n",
    "\n",
    "# 4 inputs sepal length & width, petal length & width\n",
    "inputs = np.array(iris)[:,:4].astype(np.float)\n",
    "\n",
    "# outputs are initially strings , setosa, versicolor, virginica\n",
    "outputs = np.array(iris)[:,4]\n",
    "\n",
    "#convert the out strings to ints\n",
    "outputs_vals, outputs_inds = np.unique(outputs, return_inverse=True)\n",
    "\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What are those box things? \n",
    "\n",
    "These: [1:], [:,:4], [:,4] ?\n",
    "\n",
    "Thats called slicing! It allows us to take a set of sub-elements from an array, tuple, or in our case a list without using any boring for loops! It's explained in detail [here](http://www.pythoncentral.io/how-to-slice-listsarrays-and-tuples-in-python/), pretty handy!\n",
    "\n",
    "\n",
    "### Encode and split into test & train subsets\n",
    "\n",
    "Now we're going to split the data into 2 subsets, the training set, and the testing set.\n",
    "\n",
    "###### Why?\n",
    "\n",
    "Well the reason we do this is so that we can firstly train our model with the training set, and then test it to see how well it performs using the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# endocde the category integers as binary categorial vairables.\n",
    "outputs_cats = kr.utils.to_categorical(outputs_inds)\n",
    "\n",
    "# split the input & output data sets into training and test subsets\n",
    "inds = np.random.permutation(len(inputs))\n",
    "\n",
    "train_inds, test_inds = np.array_split(inds, 2)\n",
    "\n",
    "inputs_train, outputs_train = inputs[train_inds], outputs_cats[train_inds]\n",
    "inputs_test, outputs_test = inputs[test_inds], outputs_cats[test_inds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Hold on, wheres this model you speak of?\n",
    "\n",
    "We're getting to it now, we needed to prepare the data for our model first.\n",
    "\n",
    "Now were going to create our neural network, or model as we're calling it. Ok, to build this model we'll be using Keras, a high-level API that sits on top of TensorFlow. Check out their [Documentation](https://keras.io/) for more information on why we're going to use it. But for now, all you need to know is it'll make our lives a bit easier.\n",
    "\n",
    "###### Ok! How do we do that?\n",
    "\n",
    "First we create our model, we'll use a linear stack, by assigning our model to keras.models.Sequential(). Next we add our first layer by by calling the .add() mehod and specify the amount of nodes in our hidden layer and the input shape. The model needs to know what input shape it should expect. For this reason, the first layer in a Sequential model (and only the first, because following layers can do automatic shape inference) needs to receive information about its input shape. \n",
    "\n",
    "Dense implements the operation: ```output = activation(dot(input, kernel) + bias)``` where ```activation``` is the element-wise activation function passed as the activation argument.\n",
    "\n",
    "A sigmoid function is a mathematical function having a characteristic \"S\"-shaped curve or sigmoid curve. Often, sigmoid function refers to the special case of the logistic function shown in the first figure and defined by the formula.\n",
    "![Sigmoid](https://qph.ec.quoracdn.net/main-qimg-2f0e7ccc8fd54e238ae46a3d5fcc6908?convert_to_webp=true.png)\n",
    "\n",
    "Then We'll add another layer with 3 nodes, and a final layer using Softmax activation. In mathematics, the softmax function, or normalized exponential function, is a generalization of the logistic function that \"squashes\" a K-dimensional vector of arbitrary real values to a K-dimensional vector of real values in the range [0, 1] that add up to 1.\n",
    "![Softmax](https://cdn-images-1.medium.com/max/1600/1*l6GNTFihUu0EuUMUGHMwpA.png)\n",
    "\n",
    "So by the time all thats said and done, we should have something that looks similar to this!\n",
    "![NN](https://ethervision.net/wp-content/uploads/2014/01/neural-network.png)\n",
    "\n",
    "\n",
    "### Create a neural network, add layers & nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creats a neral network\n",
    "model = kr.models.Sequential()\n",
    "\n",
    "# add an initial layer with 4 inputs nodes, and a hidden layer with 16 nodes\n",
    "model.add(kr.layers.Dense(16, input_shape=(4,)))\n",
    "\n",
    "#appy the signoid activation function to that layer\n",
    "model.add(kr.layers.Activation(\"sigmoid\"))\n",
    "\n",
    "#add another layer connected to the layet with 16 nodes containing 3 output nodes\n",
    "model.add(kr.layers.Dense(3))\n",
    "\n",
    "#use the softmax activation function there\n",
    "model.add(kr.layers.Activation(\"softmax\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the model for training, fit using training data, & evaluate using the test data\n",
    "\n",
    "First we will compile the the data using the Adam optimizer, categorical cross entropy, for multi-class classification where each example belongs to a single class.\n",
    "\n",
    "###### What is the Adam optimizer? \n",
    "\n",
    "Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent procedure to update network weights iterative based in training data. You can find out more [here](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/)\n",
    "\n",
    "We then want to go ahead and train our model, to do this we will use the train data with an epoch of 100 \n",
    "\n",
    "Then we will evaluate the test data and output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "75/75 [==============================] - 0s 3ms/step - loss: 0.1504 - acc: 0.9733\n",
      "Epoch 2/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1465 - acc: 0.9867\n",
      "Epoch 3/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1441 - acc: 0.9600\n",
      "Epoch 4/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1423 - acc: 0.9733\n",
      "Epoch 5/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1472 - acc: 0.9600\n",
      "Epoch 6/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1477 - acc: 0.9600\n",
      "Epoch 7/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1386 - acc: 0.9600\n",
      "Epoch 8/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1417 - acc: 0.9600\n",
      "Epoch 9/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1443 - acc: 0.9733\n",
      "Epoch 10/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1368 - acc: 0.9600\n",
      "Epoch 11/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1378 - acc: 0.9733\n",
      "Epoch 12/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1339 - acc: 0.9733\n",
      "Epoch 13/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1327 - acc: 0.9733\n",
      "Epoch 14/100\n",
      "75/75 [==============================] - 0s 995us/step - loss: 0.1347 - acc: 0.9600\n",
      "Epoch 15/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1330 - acc: 0.9600\n",
      "Epoch 16/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1331 - acc: 0.9733\n",
      "Epoch 17/100\n",
      "75/75 [==============================] - 0s 973us/step - loss: 0.1319 - acc: 0.9600\n",
      "Epoch 18/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1334 - acc: 0.9733\n",
      "Epoch 19/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1325 - acc: 0.9600\n",
      "Epoch 20/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1274 - acc: 0.9733\n",
      "Epoch 21/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1298 - acc: 0.9600\n",
      "Epoch 22/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1265 - acc: 0.9600\n",
      "Epoch 23/100\n",
      "75/75 [==============================] - 0s 971us/step - loss: 0.1330 - acc: 0.9467\n",
      "Epoch 24/100\n",
      "75/75 [==============================] - 0s 966us/step - loss: 0.1244 - acc: 0.9867\n",
      "Epoch 25/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1285 - acc: 0.9600\n",
      "Epoch 26/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1234 - acc: 0.9867\n",
      "Epoch 27/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1236 - acc: 0.9867\n",
      "Epoch 28/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1221 - acc: 0.9733\n",
      "Epoch 29/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1226 - acc: 0.9600\n",
      "Epoch 30/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1188 - acc: 0.9867\n",
      "Epoch 31/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1203 - acc: 0.9733\n",
      "Epoch 32/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1240 - acc: 0.9733\n",
      "Epoch 33/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1200 - acc: 0.9733\n",
      "Epoch 34/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1182 - acc: 0.9733\n",
      "Epoch 35/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1185 - acc: 0.9600\n",
      "Epoch 36/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1173 - acc: 0.9733\n",
      "Epoch 37/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1145 - acc: 0.9733\n",
      "Epoch 38/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1204 - acc: 0.9600\n",
      "Epoch 39/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1180 - acc: 0.9733\n",
      "Epoch 40/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1150 - acc: 0.9733\n",
      "Epoch 41/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1167 - acc: 0.9867\n",
      "Epoch 42/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1123 - acc: 0.9733\n",
      "Epoch 43/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1114 - acc: 0.9600\n",
      "Epoch 44/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1152 - acc: 0.9333\n",
      "Epoch 45/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1119 - acc: 0.9733\n",
      "Epoch 46/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1099 - acc: 0.9733\n",
      "Epoch 47/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1095 - acc: 0.9867\n",
      "Epoch 48/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1093 - acc: 0.9733\n",
      "Epoch 49/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1138 - acc: 0.9600\n",
      "Epoch 50/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1107 - acc: 0.9600\n",
      "Epoch 51/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1128 - acc: 0.9733\n",
      "Epoch 52/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1076 - acc: 0.9733\n",
      "Epoch 53/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1097 - acc: 0.9600\n",
      "Epoch 54/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1104 - acc: 0.9733\n",
      "Epoch 55/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1090 - acc: 0.9600\n",
      "Epoch 56/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1095 - acc: 0.9600\n",
      "Epoch 57/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1067 - acc: 0.9733\n",
      "Epoch 58/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1075 - acc: 0.9867\n",
      "Epoch 59/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1095 - acc: 0.9467\n",
      "Epoch 60/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1062 - acc: 0.9733\n",
      "Epoch 61/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1044 - acc: 0.9733\n",
      "Epoch 62/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1072 - acc: 0.9733\n",
      "Epoch 63/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1031 - acc: 0.9733\n",
      "Epoch 64/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1059 - acc: 0.9733\n",
      "Epoch 65/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1026 - acc: 0.9733\n",
      "Epoch 66/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1036 - acc: 0.9867\n",
      "Epoch 67/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1136 - acc: 0.9467\n",
      "Epoch 68/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1038 - acc: 0.9733\n",
      "Epoch 69/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1029 - acc: 0.9600\n",
      "Epoch 70/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1021 - acc: 0.9733\n",
      "Epoch 71/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1032 - acc: 0.9733\n",
      "Epoch 72/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0993 - acc: 0.9733\n",
      "Epoch 73/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1051 - acc: 0.9600\n",
      "Epoch 74/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1126 - acc: 0.9600\n",
      "Epoch 75/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1050 - acc: 0.9600\n",
      "Epoch 76/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0992 - acc: 0.9733\n",
      "Epoch 77/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0985 - acc: 0.9867\n",
      "Epoch 78/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1018 - acc: 0.9733\n",
      "Epoch 79/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0989 - acc: 0.9733\n",
      "Epoch 80/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0987 - acc: 0.9733\n",
      "Epoch 81/100\n",
      "75/75 [==============================] - 0s 929us/step - loss: 0.1044 - acc: 0.9733\n",
      "Epoch 82/100\n",
      "75/75 [==============================] - 0s 943us/step - loss: 0.1005 - acc: 0.9600\n",
      "Epoch 83/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0992 - acc: 0.9867\n",
      "Epoch 84/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1009 - acc: 0.9733\n",
      "Epoch 85/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1014 - acc: 0.9867\n",
      "Epoch 86/100\n",
      "75/75 [==============================] - 0s 999us/step - loss: 0.1039 - acc: 0.9600\n",
      "Epoch 87/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0994 - acc: 0.9600\n",
      "Epoch 88/100\n",
      "75/75 [==============================] - 0s 991us/step - loss: 0.0971 - acc: 0.9867\n",
      "Epoch 89/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0957 - acc: 0.9733\n",
      "Epoch 90/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1001 - acc: 0.9733\n",
      "Epoch 91/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1027 - acc: 0.9733\n",
      "Epoch 92/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0979 - acc: 0.9733\n",
      "Epoch 93/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0970 - acc: 0.9867\n",
      "Epoch 94/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0963 - acc: 0.9733\n",
      "Epoch 95/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0955 - acc: 0.9867\n",
      "Epoch 96/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1039 - acc: 0.9733\n",
      "Epoch 97/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.1030 - acc: 0.9467\n",
      "Epoch 98/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0960 - acc: 0.9867\n",
      "Epoch 99/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0950 - acc: 0.9733\n",
      "Epoch 100/100\n",
      "75/75 [==============================] - 0s 1ms/step - loss: 0.0963 - acc: 0.9733\n",
      "75/75 [==============================] - 0s 395us/step\n",
      "\n",
      "\n",
      "loss: 0.0565\n",
      "Accuracy: 0.9733\n"
     ]
    }
   ],
   "source": [
    "# configure the model for training\n",
    "# uses the adam optimizer and categorialcross entropy as the loss function\n",
    "# add in some extra metrics, accuracy being the only one.\n",
    "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "#fit the model using our training data\n",
    "model.fit(inputs_train, outputs_train, epochs=100, batch_size=1, verbose=1)\n",
    "#evaluate the model using the test data set\n",
    "loss, accuracy = model.evaluate(inputs_test, outputs_test, verbose=1)\n",
    "\n",
    "\n",
    "print(\"\\n\\nloss: %6.4f\\nAccuracy: %6.4f\"% (loss, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: [1 0 0]\tEstimated: [1 0 0]\n",
      "That means it's a setosa\n"
     ]
    }
   ],
   "source": [
    "# Predict the class of a single flower.\n",
    "prediction = np.around(model.predict(np.expand_dims(inputs_test[0], axis=0))).astype(np.int)[0]\n",
    "print(\"Actual: %s\\tEstimated: %s\" % (outputs_test[0].astype(np.int), prediction))\n",
    "print(\"That means it's a %s\" % outputs_vals[prediction.astype(np.bool)][0])\n",
    "\n",
    "#print(\"Error {}% Accucary {}%\".format((1.0 - accuracy)*100,(accuracy*100)))\n",
    "\n",
    "# Save the model to a file for later use.\n",
    "model.save(\"./data/iris_nn.h5\")\n",
    "# Load the model again with: model = load_model(\"iris_nn.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now with all that done we pick out a piece of data and check to see if we can predict accurately, Which it has, as shown above the Accuracy is roughly 97 - 98% which is a great result.\n",
    "\n",
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
