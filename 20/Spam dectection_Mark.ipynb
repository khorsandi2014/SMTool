{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required modules/packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate\n",
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import ShuffleSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading file and looking into the dimensions of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives around here though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label  \\\n",
       "0   ham   \n",
       "1   ham   \n",
       "2  spam   \n",
       "3   ham   \n",
       "4   ham   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
       "1                                                                        Ok lar... Joking wif u oni...  \n",
       "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
       "3                                                    U dun say so early hor... U c already then say...  \n",
       "4                                        Nah I don't think he goes to usf, he lives around here though  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data = pd.read_csv(\"SMSSpamCollection.tsv\",sep='\\t',names=['label','text'])\n",
    "pd.set_option('display.max_colwidth',100)\n",
    "raw_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.loc[:,'label']=raw_data.label.map({'ham':0,'spam':1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5572, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.865937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.134063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0     label\n",
       "label          \n",
       "0      0.865937\n",
       "1      0.134063"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(raw_data.shape)\n",
    "pd.crosstab(raw_data['label'],columns = 'label',normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST MODELS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the best model for spam detection, we can compare the standard metrics -- accuracy, precision, recall, f1 score -- for different models. We'll see below that these metrics are very good for most models. Because of that, we need to have high confidence that the metrics we are looking at are not dependent on the specific test/train split in our data. Therefore we use cross-validation and look at the average metrics across the different test/train splits. Also, we consider the standard deviation of these values, to confirm that there are no outliers in our splits. \n",
    "\n",
    "The function cross_validate does all the cross-validation for us in one simple step: making n_folds different test/train splits, fitting the training data, predicting the test data, and computing the metrics. Sweet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_metric(metric,scores):\n",
    "    test_metric='test_'+metric\n",
    "    train_metric='train_'+metric\n",
    "    print(\"Mean test/train %s: %.3f \\u00B1 %.4f / %.3f \\u00B1 %.4f\" % \\\n",
    "          (metric, scores[test_metric].mean(), scores[test_metric].std(), \\\n",
    "           scores[train_metric].mean(), scores[train_metric].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model_metrics(model,n_folds):\n",
    "    metrics=['accuracy','precision','recall','f1']\n",
    "    scores = cross_validate(model, raw_data.text, raw_data.label, scoring=metrics, cv=n_folds, return_train_score=True)\n",
    "    for metric in metrics:\n",
    "        print_metric(metric,scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NAIVE BAYES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the simplest model I can think of: Removing stop words in our vectorizer, and only using unigrams. For the ML model, let's start with a Naive Bayes model. Since our features are all binary classifiers, the best Naive Bayes model is a MultinomialNB.\n",
    "\n",
    "Notice how we're using a pipeline here. That allows us to pass the model into cross_validate: For each test/train split, it fit_transforms the training data, transform's the test data, and then runs the Naive Bayes model on the data. Nice!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test/train accuracy: 0.985 ± 0.0022 / 0.994 ± 0.0006\n",
      "Mean test/train precision: 0.962 ± 0.0233 / 0.977 ± 0.0039\n",
      "Mean test/train recall: 0.929 ± 0.0305 / 0.982 ± 0.0016\n",
      "Mean test/train f1: 0.945 ± 0.0090 / 0.979 ± 0.0022\n"
     ]
    }
   ],
   "source": [
    "nb=make_pipeline(CountVectorizer(stop_words='english'),MultinomialNB())\n",
    "print_model_metrics(nb,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's already pretty great results! What happens if we include 2-grams, 3-grams, etc.??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR MODEL WITH 1-GRAMS\n",
      "Mean test/train accuracy: 0.985 ± 0.0022 / 0.994 ± 0.0006\n",
      "Mean test/train precision: 0.962 ± 0.0233 / 0.977 ± 0.0039\n",
      "Mean test/train recall: 0.929 ± 0.0305 / 0.982 ± 0.0016\n",
      "Mean test/train f1: 0.945 ± 0.0090 / 0.979 ± 0.0022\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH 2-GRAMS\n",
      "Mean test/train accuracy: 0.987 ± 0.0032 / 0.998 ± 0.0003\n",
      "Mean test/train precision: 0.974 ± 0.0185 / 0.993 ± 0.0019\n",
      "Mean test/train recall: 0.930 ± 0.0265 / 0.995 ± 0.0010\n",
      "Mean test/train f1: 0.951 ± 0.0123 / 0.994 ± 0.0010\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH 3-GRAMS\n",
      "Mean test/train accuracy: 0.987 ± 0.0030 / 0.999 ± 0.0001\n",
      "Mean test/train precision: 0.981 ± 0.0154 / 0.997 ± 0.0008\n",
      "Mean test/train recall: 0.925 ± 0.0240 / 0.997 ± 0.0006\n",
      "Mean test/train f1: 0.952 ± 0.0118 / 0.997 ± 0.0005\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH 4-GRAMS\n",
      "Mean test/train accuracy: 0.987 ± 0.0032 / 0.999 ± 0.0001\n",
      "Mean test/train precision: 0.984 ± 0.0117 / 0.997 ± 0.0007\n",
      "Mean test/train recall: 0.921 ± 0.0256 / 0.999 ± 0.0004\n",
      "Mean test/train f1: 0.951 ± 0.0129 / 0.998 ± 0.0004\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n_max in range(1,5):\n",
    "    print(\"RESULTS FOR MODEL WITH %s-GRAMS\" % n_max)\n",
    "    nb=make_pipeline(CountVectorizer(stop_words='english',ngram_range=(1, n_max)),MultinomialNB())\n",
    "    print_model_metrics(nb,10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding 2-grams seems to help the precision a bit, but after that there's not much effect on the test metrics. Let's limit our models to 2-grams.\n",
    "\n",
    "What about limiting the number of features, either with max_features or min_df?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR MODEL WITH min_df=1\n",
      "Mean test/train accuracy: 0.987 ± 0.0032 / 0.998 ± 0.0003\n",
      "Mean test/train precision: 0.974 ± 0.0185 / 0.993 ± 0.0019\n",
      "Mean test/train recall: 0.930 ± 0.0265 / 0.995 ± 0.0010\n",
      "Mean test/train f1: 0.951 ± 0.0123 / 0.994 ± 0.0010\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH min_df=2\n",
      "Mean test/train accuracy: 0.985 ± 0.0032 / 0.992 ± 0.0006\n",
      "Mean test/train precision: 0.967 ± 0.0225 / 0.983 ± 0.0031\n",
      "Mean test/train recall: 0.922 ± 0.0253 / 0.957 ± 0.0024\n",
      "Mean test/train f1: 0.944 ± 0.0123 / 0.970 ± 0.0022\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH min_df=3\n",
      "Mean test/train accuracy: 0.985 ± 0.0018 / 0.989 ± 0.0005\n",
      "Mean test/train precision: 0.963 ± 0.0223 / 0.972 ± 0.0029\n",
      "Mean test/train recall: 0.922 ± 0.0252 / 0.947 ± 0.0031\n",
      "Mean test/train f1: 0.942 ± 0.0073 / 0.959 ± 0.0021\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH min_df=4\n",
      "Mean test/train accuracy: 0.983 ± 0.0023 / 0.988 ± 0.0006\n",
      "Mean test/train precision: 0.961 ± 0.0223 / 0.968 ± 0.0030\n",
      "Mean test/train recall: 0.912 ± 0.0276 / 0.939 ± 0.0036\n",
      "Mean test/train f1: 0.935 ± 0.0095 / 0.954 ± 0.0024\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH min_df=5\n",
      "Mean test/train accuracy: 0.983 ± 0.0024 / 0.987 ± 0.0006\n",
      "Mean test/train precision: 0.958 ± 0.0234 / 0.966 ± 0.0033\n",
      "Mean test/train recall: 0.913 ± 0.0312 / 0.933 ± 0.0039\n",
      "Mean test/train f1: 0.934 ± 0.0100 / 0.949 ± 0.0025\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,6):\n",
    "    print(\"RESULTS FOR MODEL WITH min_df=%s\" % n)\n",
    "    nb=make_pipeline(CountVectorizer(stop_words='english',ngram_range=(1, 2), min_df=n),MultinomialNB())\n",
    "    print_model_metrics(nb,10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37364"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# With 2-grams, how many features do we have?\n",
    "\n",
    "cv=CountVectorizer(stop_words='english',ngram_range=(1, 2))\n",
    "X=cv.fit_transform(raw_data.text)\n",
    "X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR MODEL WITH max_features=5000\n",
      "Mean test/train accuracy: 0.985 ± 0.0027 / 0.990 ± 0.0006\n",
      "Mean test/train precision: 0.966 ± 0.0220 / 0.975 ± 0.0029\n",
      "Mean test/train recall: 0.925 ± 0.0262 / 0.952 ± 0.0030\n",
      "Mean test/train f1: 0.945 ± 0.0107 / 0.963 ± 0.0022\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=10000\n",
      "Mean test/train accuracy: 0.985 ± 0.0027 / 0.993 ± 0.0004\n",
      "Mean test/train precision: 0.968 ± 0.0199 / 0.985 ± 0.0029\n",
      "Mean test/train recall: 0.922 ± 0.0244 / 0.965 ± 0.0023\n",
      "Mean test/train f1: 0.944 ± 0.0105 / 0.975 ± 0.0016\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=15000\n",
      "Mean test/train accuracy: 0.986 ± 0.0029 / 0.996 ± 0.0007\n",
      "Mean test/train precision: 0.971 ± 0.0184 / 0.987 ± 0.0028\n",
      "Mean test/train recall: 0.925 ± 0.0268 / 0.979 ± 0.0030\n",
      "Mean test/train f1: 0.947 ± 0.0115 / 0.983 ± 0.0026\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=20000\n",
      "Mean test/train accuracy: 0.987 ± 0.0026 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.973 ± 0.0178 / 0.990 ± 0.0023\n",
      "Mean test/train recall: 0.928 ± 0.0261 / 0.985 ± 0.0017\n",
      "Mean test/train f1: 0.949 ± 0.0103 / 0.988 ± 0.0013\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=25000\n",
      "Mean test/train accuracy: 0.987 ± 0.0024 / 0.997 ± 0.0003\n",
      "Mean test/train precision: 0.975 ± 0.0179 / 0.992 ± 0.0020\n",
      "Mean test/train recall: 0.925 ± 0.0247 / 0.988 ± 0.0012\n",
      "Mean test/train f1: 0.949 ± 0.0097 / 0.990 ± 0.0011\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=30000\n",
      "Mean test/train accuracy: 0.987 ± 0.0029 / 0.998 ± 0.0003\n",
      "Mean test/train precision: 0.973 ± 0.0190 / 0.993 ± 0.0019\n",
      "Mean test/train recall: 0.930 ± 0.0265 / 0.994 ± 0.0012\n",
      "Mean test/train f1: 0.951 ± 0.0113 / 0.993 ± 0.0013\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=35000\n",
      "Mean test/train accuracy: 0.987 ± 0.0032 / 0.998 ± 0.0003\n",
      "Mean test/train precision: 0.974 ± 0.0185 / 0.993 ± 0.0019\n",
      "Mean test/train recall: 0.930 ± 0.0265 / 0.995 ± 0.0010\n",
      "Mean test/train f1: 0.951 ± 0.0123 / 0.994 ± 0.0010\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH max_features=40000\n",
      "Mean test/train accuracy: 0.987 ± 0.0032 / 0.998 ± 0.0003\n",
      "Mean test/train precision: 0.974 ± 0.0185 / 0.993 ± 0.0019\n",
      "Mean test/train recall: 0.930 ± 0.0265 / 0.995 ± 0.0010\n",
      "Mean test/train f1: 0.951 ± 0.0123 / 0.994 ± 0.0010\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,9):\n",
    "    max_features=n*5000\n",
    "    print(\"RESULTS FOR MODEL WITH max_features=%s\" % max_features)\n",
    "    nb=make_pipeline(CountVectorizer(stop_words='english',ngram_range=(1, 2), \\\n",
    "                                     max_features=max_features),MultinomialNB())\n",
    "    print_model_metrics(nb,10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limiting the number of features using min_df and max_features can serve two purposes:\n",
    "\n",
    "1. Preventing overfitting: We can assess this by looking at how the test and train metrics compare. An overfitted model will have much better training metrics than testing metrics. \n",
    "2. Improving the results of the model.\n",
    "\n",
    "\n",
    "Change min_df only seems to hurt the test and train metrics equally, so that does not seem like a good way to limit the number of features. \n",
    "\n",
    "Reducing max_features, on the other hand, has hardly any effect on the test metrics at first. The metrics for max_features = 20,000 are essentially equal to the unlimited model (max_features=40,000). The training metrics are slightly worse as we decrease max_features, which means that we are avoiding overfitting the training set. Therefore, we can set max_features=20,000 without hurting the quality of the test results, while at the same time producing a less over-fitted model. \n",
    "\n",
    "\n",
    "**CONCLUSION**: The \"best\" Vectorizer for this dataset is to use 2-grams and 1-grams, and to limit the features to 20,000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about hyperparameters for the Naive Bayes model? The only one that is worth adjusting is alpha, the smoothing parameter. Let's see what happens as we adjust that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULTS FOR MODEL WITH alpha=0.1\n",
      "Mean test/train accuracy: 0.986 ± 0.0034 / 0.997 ± 0.0002\n",
      "Mean test/train precision: 0.958 ± 0.0199 / 0.986 ± 0.0012\n",
      "Mean test/train recall: 0.938 ± 0.0241 / 0.995 ± 0.0010\n",
      "Mean test/train f1: 0.948 ± 0.0130 / 0.990 ± 0.0007\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.2\n",
      "Mean test/train accuracy: 0.987 ± 0.0031 / 0.997 ± 0.0003\n",
      "Mean test/train precision: 0.961 ± 0.0222 / 0.985 ± 0.0016\n",
      "Mean test/train recall: 0.938 ± 0.0241 / 0.994 ± 0.0014\n",
      "Mean test/train f1: 0.949 ± 0.0119 / 0.990 ± 0.0010\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.3\n",
      "Mean test/train accuracy: 0.987 ± 0.0035 / 0.997 ± 0.0002\n",
      "Mean test/train precision: 0.965 ± 0.0203 / 0.985 ± 0.0012\n",
      "Mean test/train recall: 0.940 ± 0.0256 / 0.992 ± 0.0014\n",
      "Mean test/train f1: 0.952 ± 0.0131 / 0.989 ± 0.0008\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.4\n",
      "Mean test/train accuracy: 0.987 ± 0.0031 / 0.997 ± 0.0002\n",
      "Mean test/train precision: 0.966 ± 0.0177 / 0.984 ± 0.0013\n",
      "Mean test/train recall: 0.940 ± 0.0256 / 0.991 ± 0.0015\n",
      "Mean test/train f1: 0.952 ± 0.0120 / 0.988 ± 0.0007\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.5\n",
      "Mean test/train accuracy: 0.988 ± 0.0032 / 0.997 ± 0.0002\n",
      "Mean test/train precision: 0.969 ± 0.0194 / 0.985 ± 0.0015\n",
      "Mean test/train recall: 0.940 ± 0.0256 / 0.990 ± 0.0014\n",
      "Mean test/train f1: 0.954 ± 0.0122 / 0.988 ± 0.0008\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.6\n",
      "Mean test/train accuracy: 0.987 ± 0.0032 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.969 ± 0.0195 / 0.985 ± 0.0027\n",
      "Mean test/train recall: 0.937 ± 0.0239 / 0.990 ± 0.0017\n",
      "Mean test/train f1: 0.952 ± 0.0123 / 0.987 ± 0.0015\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.7\n",
      "Mean test/train accuracy: 0.987 ± 0.0030 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.969 ± 0.0221 / 0.986 ± 0.0025\n",
      "Mean test/train recall: 0.932 ± 0.0282 / 0.989 ± 0.0015\n",
      "Mean test/train f1: 0.949 ± 0.0118 / 0.987 ± 0.0016\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.8\n",
      "Mean test/train accuracy: 0.986 ± 0.0024 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.971 ± 0.0191 / 0.987 ± 0.0023\n",
      "Mean test/train recall: 0.926 ± 0.0255 / 0.987 ± 0.0014\n",
      "Mean test/train f1: 0.948 ± 0.0098 / 0.987 ± 0.0015\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=0.9\n",
      "Mean test/train accuracy: 0.987 ± 0.0026 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.973 ± 0.0178 / 0.989 ± 0.0027\n",
      "Mean test/train recall: 0.928 ± 0.0261 / 0.986 ± 0.0015\n",
      "Mean test/train f1: 0.949 ± 0.0103 / 0.987 ± 0.0016\n",
      "\n",
      "\n",
      "RESULTS FOR MODEL WITH alpha=1.0\n",
      "Mean test/train accuracy: 0.987 ± 0.0026 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.973 ± 0.0178 / 0.990 ± 0.0023\n",
      "Mean test/train recall: 0.928 ± 0.0261 / 0.985 ± 0.0017\n",
      "Mean test/train f1: 0.949 ± 0.0103 / 0.988 ± 0.0013\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for n in range(1,11):\n",
    "    alpha=n*0.1\n",
    "    print(\"RESULTS FOR MODEL WITH alpha=%.1f\" % alpha)\n",
    "    nb=make_pipeline(CountVectorizer(stop_words='english',ngram_range=(1, 2), \\\n",
    "                                     max_features=20000),MultinomialNB(alpha=alpha))\n",
    "    print_model_metrics(nb,10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "alpha=0.5 has the best overall metrics; specifically, the f1 score is the highest. The differences are minimal, but we might as well use the best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FINAL NAIVE BAYES MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean test/train accuracy: 0.988 ± 0.0032 / 0.998 ± 0.0002\n",
      "Mean test/train precision: 0.970 ± 0.0153 / 0.989 ± 0.0013\n",
      "Mean test/train recall: 0.940 ± 0.0269 / 0.997 ± 0.0006\n",
      "Mean test/train f1: 0.954 ± 0.0125 / 0.993 ± 0.0008\n"
     ]
    }
   ],
   "source": [
    "nb=make_pipeline(CountVectorizer(stop_words='english', max_features=40000, ngram_range=(1,2)),MultinomialNB(alpha=0.5))\n",
    "print_model_metrics(nb,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RANDOM FOREST MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, I also ran a Random Forest model on the data. I didn't have time to exhaustively optimize the hyperparameters, as I decided to focus on the Titanic dataset for my presentation. However, I did investigate the effect of max_depth on the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=10\n",
      "Mean test/train accuracy: 0.952 ± 0.0081 / 0.963 ± 0.0018\n",
      "Mean test/train precision: 0.933 ± 0.0393 / 0.992 ± 0.0044\n",
      "Mean test/train recall: 0.689 ± 0.0545 / 0.733 ± 0.0133\n",
      "Mean test/train f1: 0.791 ± 0.0388 / 0.843 ± 0.0087\n",
      "\n",
      "\n",
      "max_depth=20\n",
      "Mean test/train accuracy: 0.966 ± 0.0075 / 0.982 ± 0.0008\n",
      "Mean test/train precision: 0.933 ± 0.0401 / 0.994 ± 0.0015\n",
      "Mean test/train recall: 0.802 ± 0.0503 / 0.873 ± 0.0061\n",
      "Mean test/train f1: 0.861 ± 0.0314 / 0.929 ± 0.0033\n",
      "\n",
      "\n",
      "max_depth=30\n",
      "Mean test/train accuracy: 0.969 ± 0.0068 / 0.988 ± 0.0011\n",
      "Mean test/train precision: 0.930 ± 0.0346 / 0.993 ± 0.0017\n",
      "Mean test/train recall: 0.830 ± 0.0464 / 0.916 ± 0.0079\n",
      "Mean test/train f1: 0.876 ± 0.0280 / 0.953 ± 0.0043\n",
      "\n",
      "\n",
      "max_depth=40\n",
      "Mean test/train accuracy: 0.969 ± 0.0060 / 0.992 ± 0.0009\n",
      "Mean test/train precision: 0.926 ± 0.0318 / 0.993 ± 0.0023\n",
      "Mean test/train recall: 0.835 ± 0.0438 / 0.946 ± 0.0068\n",
      "Mean test/train f1: 0.877 ± 0.0251 / 0.969 ± 0.0037\n",
      "\n",
      "\n",
      "max_depth=None\n",
      "Mean test/train accuracy: 0.972 ± 0.0052 / 0.997 ± 0.0004\n",
      "Mean test/train precision: 0.947 ± 0.0299 / 0.997 ± 0.0010\n",
      "Mean test/train recall: 0.839 ± 0.0426 / 0.978 ± 0.0027\n",
      "Mean test/train f1: 0.889 ± 0.0218 / 0.987 ± 0.0017\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_depths=(10,20,30,40,None)\n",
    "\n",
    "for max_depth in max_depths:\n",
    "    rf=make_pipeline(CountVectorizer(stop_words='english', max_features=40000, ngram_range=(1,2)),\\\n",
    "                 RandomForestClassifier(random_state=1212, n_jobs=-1,max_features=None,max_depth=max_depth))\n",
    "    print('max_depth=%s' % max_depth)\n",
    "    print_model_metrics(rf,10)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model metrics are not as good as the Naive Bayes model. That's not surprising; Naive Bayes is known as an excellent model for Spam detection. Specifically, the Random Forest model does poorly on recall, i.e. correctly identifying all the spam emails. \n",
    "\n",
    "Furthermore, the Random Forest model suffers from overfitting. As the tree size increases, the difference between the test and train metrics -- particularly the recall -- grows. Unfortunately, the test metrics also get worse as the tree size decreases. So we have to balance that out in choosing a final model. I think max_depth=20 strikes a nice balance: The test recall is 80%, while the training recall is 87%. Compare that 7% difference to the 15% difference you get for max_depth=None. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
